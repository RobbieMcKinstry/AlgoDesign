\documentclass[12pt]{article}

\usepackage{tikz}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{pst-node,pst-plot}
\usepackage{amsmath}
\usepackage{float}

\newcommand{\BigO}[1]{\ensuremath{\operatorname{\mathcal{O}}\bigl(#1\bigr)}}

\begin{document}
\title{Homework 20}
\author{Robbie McKinstry, Jack McQuown, Cyrus Ramavarapu}
\renewcommand{\today}{18 September 2016}
\renewcommand{\baselinestretch}{1.5}
\maketitle

\section*{Reductions}
\subsection*{Problem 19: }
\subsection*{Problem 22: }

This problem is vertex cover in disguise! My reasoning is that I tried a bunch of different graphs and anything that worked for vertex cover also worked for the River Crossing problem, and anything that worked for the River Crossing problem also worked for vertex cover.

More formally, given a graph $G$, the first boat-trip will take $k$ resources across the bridge and thus covering $k$ edges; if at that time there remains an uncovered edge, then the resources devour each other and both the River Crossing problem and vertex cover fail to find a solution. Conversely, if there are no uncovered edges, then there are no mutually exclusive resources. Additionally, because each return trip acts on a subgraph of $G$, then any satisfying trip implies the existence of an additional satisfying trip, since there are no more vertices in a subgraph than there are in the graph from which the subgraph is derived. 

Finally, we must show that if there exists a safe first trip, then all following trips can ultimately terminate successfully. Each trip can move at least one resources across the bridge without introducing conflict on the other side (since no edges are left uncovered as stated above). Let $k$ be the number of resources the boat is holding and $j$ be the number of resources on the original side of the bridge. The first trip can set down at least 1 resource on the other side of the bridge safely, since a single vertex has a degree of 0, and because the initial side of the bridge is fully covered by the assumption that there exists a safe first trip. At this point in time, the boat holds at most $k-1$ resources and there are still $j$ resources left on the initial side. The boat returns, and can pick up another resource on the initial side, which obviously won't introduce any uncovered edges in the new subgraph of $j-1$ resources since you're removing a vertex from the graph. Now, upon return to the destination-side of the river, you can place down the most recently received resource and pick up the first resource placed. Now, the origin-side of the river contains $j-1$ resources and the destination-side contains $1$ resource, while the boat contains $k$ resources. This process repeats, transferring at least one resource at a time across the river until all $j$ resources are successfully transferred. Both sides of the river are always satisfied because there make up a partition of the $j$ resources, which themselves are disjoint, so any two subsets of two disjoint sets are themselves disjoint. Thus, River Crossing can terminate if there is an initial satisfying trip.

Since vertex cover on a graph $G$ with covering vertices $k$ is satisfiable if and only if River Crossing on $G$ with a boat of size $K$ is satisfiable, then vertex cover reduces to River Crossing (and River Crossing reduces to vertex cover). This implies that River Crossing is NP-Hard since an NP-Complete problem reduces to it.

\begin{algorithm}[H]
\SetKw{Func}{Function:}
\SetKw{Inp}{Input:}
\SetKw{Glob}{Globals:}
\SetKw{Ret}{Return:}
\Func{Vertex Cover}\\
\Inp{Graph $G$, Integer $k$}\\
\Ret{RiverCrossing($G, k$)}
\end{algorithm}

\section*{Parallel}
\subsection*{Problem 1: }
\subsubsection*{a}
The AND of $n$ bits can be solved in \BigO{\log n} time on an
EREW PRAM using the following divide and conquer algorithm.\\\\
\begin{algorithm}[H]
\SetKw{Func}{Function:}
\SetKw{Inp}{Input:}
\SetKw{Glob}{Globals:}
\SetKw{Ret}{Return:}
\Func{$AND\_EREW\_1$}\\
\Inp{$Bits\ b_a\_b_n$,$Processors\ n$}\\
\If{$n==1$}
{\Ret{$b$}}
\Ret{$AND\_EREW\_1(b_1\dots b_{n/2},n/2)\land AND\_EREW\_1(b_{n/2 + 1}\dots b_{n},n/2)$}
\end{algorithm}
This algorithm takes \BigO{\log n} time because each recursive
call halves the problem and the call tree continues down until 
each processor has a single bit whereupon it is a constant
time operation to perform an AND.\\\\
The efficiency of this algorithm is defined as:
\[
E=\frac{T_{serial}(n,p)}{pT_{parallel}(n,p)}
\]
Since there are n processors in this problem, the actual running
time for the parallel algorithm can be solved as $T(n,n) = 1 + \log(n)$.
Solving for the efficiency using the sertial run time of $T(n,1) = n$
gives $E=\frac{n}{1+n\log(n)}$.\\\\
Using the folding principle, an upper bound to the runtime of the parallel
algorithm if $n^{1/3}$ processors are used to be $T(n,n^{1/3})=n^{2/3}(1+\log(n))$.
\subsubsection*{b}
Similar to the algorithm for $AND\_EREW\_1$, the same divide an conquer
strategy can be used to develop a parallel algorithm for the AND problem
using only $\frac{n}{\log n}$ processors that still runs in \BigO{\log n}
time.  The reason the same strategy works is that although the recursive
tree for divide and conquer will have fewer levels and each processor
will now do more work, the work done by each processor in parallel will
now dominate.  This work is bounded by $\frac{n}{n\log n}$ or
equivalently $\log n$ for each processor.  Algorithmically this process can be written
as follows.\\\\
\begin{algorithm}[H]
\SetKw{Func}{Function:}
\SetKw{Inp}{Input:}
\SetKw{Glob}{Globals:}
\SetKw{Ret}{Return:}
\Func{$AND\_EREW\_2$}\\
\Inp{$Bits\ b_a\_b_n$,$Processors\ n$}\\
\If{$n==1$}
{$temp=1$\\
\For{$Bit\ b\ in [b_a,b_n]$}
{$temp=temp\land b$}
}
\Ret{$AND\_EREW\_2(b_1\dots b_{n/2},n/2)\land AND\_EREW\_2(b_{n/2 + 1}\dots b_{n},n/2)$}
\end{algorithm}
The efficiency of this algorithm can be calculated in the same way as
$AND\_EREW\_1$ to give $E=\frac{n}{n+p\log p}$ with $p$ set to $\frac{n}{\log n}$
this gives an asymptotic efficiency of $1$.\\\\
Using the folding principle an upper bound to the running time of this algorithm
with $n^{2/3}$ processors can be determined because the running time of the parallel
program will increase by a factor of at most $n^{2/3}$.  This gives the following 
run time: $T(n,n^{1/3})\leq n^{2/3}(\log n + \log(n/\log n))$.
\subsubsection*{b}
Transitioning to an algorithm for the AND problem that runs on a CRCW PRAM in
\BigO{1} using $n$ processors requires a fundamental change in the algorithm
in comparison to the $AND\_EREW$ algorithms.  This is accomplished by 
giving each of the $n$ processors an individual bit.  If the bit belonging to
the processor is a $0$, the processor will write to the shared result memory
location a $0$.  All of these writes will be done in parallel and and hence
the algorithm will take \BigO{1}.\\\\
\begin{algorithm}[H]
\SetKw{Func}{Function:}
\SetKw{Inp}{Input:}
\SetKw{Glob}{Globals:}
\SetKw{Ret}{Return:}
\Func{$AND\_CRCW$}\\
\Inp{$Bits\ b_a\_b_n$,$Processors\ n$}\\
\tcc{Result exists in shared memory}
$Result = 1$
\tcc{Each Processor gets a bit}
\If{$b==0$}
{$Result=0$}
\end{algorithm}
This algorithm operates on the weakest type of CRCW PRAM, COMMON, because if a processor
writes to the shared memory location, it writes the same value as any other
processor that writes to the shared location.\\\\
The efficiency of this algorithm can be computed as before by taking
the ratio of the speed up to the number of processors.  Since there are
$n$ processors, the efficiency is $1$\\\\
By the folding principle an upper bound on this algorithm if only $n^{2/3}$
processors are used.  In this case the time will be $T(n, n^{2/3}) = n^{1/3}$   
\end{document}
